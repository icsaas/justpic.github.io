<!doctype html><html amp lang=zh-cn class=no-js><head><title>计算机视觉</title><meta charset=utf-8><script async custom-element=amp-ad src=https://cdn.ampproject.org/v0/amp-ad-0.1.js></script><script async custom-element=amp-analytics src=https://cdn.ampproject.org/v0/amp-analytics-0.1.js></script><script async custom-element=amp-audio src=https://cdn.ampproject.org/v0/amp-audio-0.1.js></script><script async src=https://cdn.ampproject.org/v0.js></script><link rel=canonical href=https://justpic.org/posts/2014/05/06/ai-collection-2014/><link rel=icon href=/favicon.png type=image/png sizes=144x144><meta name=viewport content="width=device-width,initial-scale=1,minimum-scale=1"><meta name=theme-color content="#000000"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"桑河一榆","url":"https:\/\/justpic.org\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"https:\/\/justpic.org\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/justpic.org\/","name":"home"}},{"@type":"ListItem","position":2,"item":{"@id":"https:\/\/justpic.org\/amp\/","name":"amp"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/justpic.org\/amp\/posts\/","name":"posts"}},{"@type":"ListItem","position":4,"item":{"@id":"https:\/\/justpic.org\/amp\/posts\/2014\/","name":"2014"}},{"@type":"ListItem","position":5,"item":{"@id":"https:\/\/justpic.org\/amp\/posts\/2014\/05\/","name":"05"}},{"@type":"ListItem","position":6,"item":{"@id":"https:\/\/justpic.org\/amp\/posts\/2014\/05\/06\/","name":"06"}},{"@type":"ListItem","position":7,"item":{"@id":"https:\/\/justpic.org\/amp\/posts\/2014\/05\/06\/ai-collection-2014\/","name":"ai-collection-2014"}}]}</script><style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript><style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background:0 0}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type=checkbox],input[type=radio]{box-sizing:border-box;padding:0}input[type=number]::-webkit-inner-spin-button,input[type=number]::-webkit-outer-spin-button{height:auto}input[type=search]{-webkit-appearance:textfield;box-sizing:content-box}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:700}td,th{padding:0}*{box-sizing:border-box}button,input,select,textarea{font:13px/1.4 Helvetica,arial,freesans,clean,sans-serif}a{color:#4183c4;text-decoration:none}a:active,a:focus,a:hover{text-decoration:underline}.rule,hr{height:0;margin:15px 0;overflow:hidden;background:0 0;border:0;border-bottom:1px solid #ddd}.rule:after,.rule:before,hr:after,hr:before{display:table;content:""}.rule:after,hr:after{clear:both}fieldset{padding:0;margin:0;border:0}label{font-size:13px;font-weight:700}#adv_code_search .search-page-label,input[type=email],input[type=number],input[type=password],input[type=tel],input[type=text],input[type=url],textarea{min-height:34px;padding:7px 8px;font-size:13px;color:#333;vertical-align:middle;background-color:#fff;background-repeat:no-repeat;background-position:100%;border:1px solid #ccc;border-radius:3px;outline:none;box-shadow:inset 0 1px 2px rgba(0,0,0,.075)}#adv_code_search .focus.search-page-label,#adv_code_search .search-page-label:focus,.focused .drag-and-drop,input[type=email].focus,input[type=email]:focus,input[type=number].focus,input[type=number]:focus,input[type=password].focus,input[type=password]:focus,input[type=tel].focus,input[type=tel]:focus,input[type=text].focus,input[type=text]:focus,input[type=url].focus,input[type=url]:focus,textarea.focus,textarea:focus{border-color:#51a7e8;box-shadow:inset 0 1px 2px rgba(0,0,0,.075),0 0 5px rgba(81,167,232,.5)}.input-contrast,input.input-contrast{background-color:#fafafa}.input-contrast:focus,input.input-contrast:focus{background-color:#fff}:-moz-placeholder,::-webkit-input-placeholder{color:#aaa}::-webkit-validation-bubble-message{font-size:12px;color:#fff;background:#9c2400;border:0;border-radius:3px;-webkit-box-shadow:1px 1px 1px rgba(0,0,0,.1)}input::-webkit-validation-bubble-icon{display:none}::-webkit-validation-bubble-arrow{background-color:#9c2400;border:1px solid #9c2400;-webkit-box-shadow:1px 1px 1px rgba(0,0,0,.1)}.absent{color:#c00}.anchor{position:absolute;top:0;bottom:0;left:0;display:block;padding-right:6px;padding-left:30px;margin-left:-30px}.anchor:focus{outline:none}h1,h2,h3,h4,h5,h6{position:relative;margin-top:1em;margin-bottom:16px;font-weight:700;line-height:1.4}h1 .octicon-link,h2 .octicon-link,h3 .octicon-link,h4 .octicon-link,h5 .octicon-link,h6 .octicon-link{display:none;color:#000;vertical-align:middle}h1:hover .anchor,h2:hover .anchor,h3:hover .anchor,h4:hover .anchor,h5:hover .anchor,h6:hover .anchor{height:1em;padding-left:8px;margin-left:-30px;line-height:1;text-decoration:none}h1:hover .anchor .octicon-link,h2:hover .anchor .octicon-link,h3:hover .anchor .octicon-link,h4:hover .anchor .octicon-link,h5:hover .anchor .octicon-link,h6:hover .anchor .octicon-link{display:inline-block}h1 code,h1 tt,h2 code,h2 tt,h3 code,h3 tt,h4 code,h4 tt,h5 code,h5 tt,h6 code,h6 tt{font-size:inherit}h1{font-size:2.25em;line-height:1.2}h1,h2{padding-bottom:.3em;border-bottom:1px solid #eee}h2{font-size:1.75em;line-height:1.225}h3{font-size:1.5em;line-height:1.43}h4{font-size:1.25em}h5,h6{font-size:1em}h6{color:#777}blockquote,dl,ol,p,pre,table,ul{margin-top:0;margin-bottom:16px}hr{height:4px;padding:0;margin:16px 0;background-color:#e7e7e7;border:0}ol,ul{padding-left:2em}ol.no-list,ul.no-list{padding:0;list-style-type:none}ol ol,ol ul,ul ol,ul ul{margin-top:0;margin-bottom:0}li>p{margin-top:16px}dl,dl dt{padding:0}dl dt{margin-top:16px;font-size:1em;font-style:italic;font-weight:700}dl dd{padding:0 16px;margin-bottom:16px}blockquote{padding:0 15px;color:#777;border-left:4px solid #ddd}blockquote>:first-child{margin-top:0}blockquote>:last-child{margin-bottom:0}table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}table th{font-weight:700}table td,table th{padding:6px 13px;border:1px solid #ddd}table tr{background-color:#fff;border-top:1px solid #ccc}table tr:nth-child(2n){background-color:#f8f8f8}img{max-width:100%;box-sizing:border-box}span.frame,span.frame>span{display:block;overflow:hidden}span.frame>span{float:left;width:auto;padding:7px;margin:13px 0 0;border:1px solid #ddd}span.frame span img{display:block;float:left}span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}span.align-center{display:block;overflow:hidden;clear:both}span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}span.align-center span img{margin:0 auto;text-align:center}span.align-right{display:block;overflow:hidden;clear:both}span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}span.align-right span img{margin:0;text-align:right}span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}span.float-left span{margin:13px 0 0}span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}code,tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}code:after,code:before,tt:after,tt:before{letter-spacing:-.2em;content:"\00a0"}code br,tt br{display:none}del code{text-decoration:inherit;vertical-align:text-top}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.highlight{margin-bottom:16px}.highlight pre,pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.highlight pre{margin-bottom:0;word-break:normal}pre,pre code,pre tt{word-wrap:normal}pre code,pre tt{display:inline;max-width:none;padding:0;margin:0;overflow:initial;line-height:inherit;background-color:transparent;border:0}pre code:after,pre code:before,pre tt:after,pre tt:before{content:normal}abbr,address,article,aside,audio,b,blockquote,body,canvas,caption,cite,code,dd,del,details,dfn,div,dl,dt,em,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,i,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,p,pre,q,samp,section,small,span,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,ul,var,video{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:0 0}body{line-height:1}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}nav ul{list-style:none}blockquote,q{quotes:none}blockquote:after,blockquote:before,q:after,q:before{content:'';content:none}a{margin:0;padding:0;font-size:100%;vertical-align:baseline;background:0 0}ins{text-decoration:none}ins,mark{background-color:#ff9;color:#000}mark{font-style:italic;font-weight:700}del{text-decoration:line-through}abbr[title],dfn[title]{border-bottom:1px dotted;cursor:help}table{border-collapse:collapse;border-spacing:0}hr{display:block;height:1px;border:0;border-top:1px solid #ccc;margin:1em 0;padding:0}input,select{vertical-align:middle}body{background-color:#fff;color:#123;font-size:14px;font-family:Helvetica Neue,Helvetica,Segoe UI,Arial,freesans,sans-serif}main{max-width:1180px;padding:30px;margin:0 auto}@media(min-width:900px){main h1+p{font-size:2em}}h1{font-weight:300;font-size:2em;line-height:1.4em;margin:.5em 0;font-family:Georgia,serif;-webkit-transition:color .4s;transition:color .4s}h2{font-size:2em}h2,h3{font-weight:300;margin:30px 0}h3{font-size:1.8em}h4{font-size:1.6em}h4,h5{font-weight:300}h5{font-size:1.2em}h6{font-weight:300;font-size:1em}p{line-height:1.4em;font-size:1.2em;margin:15px 0}blockquote{border-left:5px solid #ff1d55;background-color:#ffe9ee;padding:1em;margin-bottom:1em}.lightbox{background-color:rgba(3,121,196,.4);min-height:100vh;min-width:100vw;padding:30px}code{font:12px Consolas,Liberation Mono,Menlo,Courier,monospace}p>code{color:#ff1d55}pre code{display:block;background-color:#f7f7f7;padding:15px;overflow:auto;color:#123;line-height:1.4em}a{color:#ff1d55;text-decoration:none;-webkit-transition:color .4s,background .4s;transition:color .4s,background .4s}a:hover{color:#e9003a}ol,ul{margin:7.5px;padding:7.5px 15px}ol li,ul li{margin:7.5px 15px}.doc-parameters{list-style-type:none;padding:0}.doc-parameters li{display:block;margin:0;margin-bottom:15px}.doc-parameters .example,.doc-parameters blockquote{border-left:5px solid #0379c4;background-color:#96d5fd;color:#123;line-height:1em;font-size:1em;padding:1em;margin:0}.doc-parameters .example{background-color:#c8e9fe}.doc-parameters .parameter{display:block;padding:1em;background-color:#0379c4;color:#fff;margin:0}.doc-parameters .parameter strong{display:inline-block;padding:0 .5em;border-radius:.25em;line-height:2em;background-color:#025a92}hr{height:.25em;padding:0;margin:24px 0;background-color:#f7f7f7;border:0}.teaser{display:-webkit-box;display:-ms-flexbox;display:flex;text-align:left;-ms-flex-line-pack:center;align-content:center;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.teaser aside{-ms-flex-preferred-size:60%;flex-basis:60%;-ms-flex-item-align:center;-ms-grid-row-align:center;align-self:center;padding:30px}@media(max-width:700px){.teaser aside{-ms-flex-preferred-size:100%;flex-basis:100%}}.teaser .content{-ms-flex-preferred-size:40%;flex-basis:40%;-ms-flex-item-align:center;-ms-grid-row-align:center;align-self:center;color:dimgray;font-size:12px}@media(max-width:700px){.teaser .content{-ms-flex-preferred-size:100%;flex-basis:100%}}.teaser .content.content-full{-ms-flex-preferred-size:100%;flex-basis:100%}.teaser .content h3{font-size:14px;text-transform:uppercase;font-weight:600;color:#123}.main-index{text-align:center}.main-index amp-img{margin:15px auto;width:50%}@media(min-width:900px){.main-index p{font-size:2em}}.breadcrumbs{padding:1em;background-color:#f7f7f7}.breadcrumbs ul{display:block;margin:0;padding:0}.breadcrumbs ul li{display:inline-block;margin:0 .5em}.breadcrumbs ul li:before{content:' / ';position:relative;margin-left:-.5em;color:dimgray}.breadcrumbs ul li a{color:dimgray}.breadcrumbs ul li:last-child a{color:#ff1d55}header{background-color:#000;color:#fff;padding:30px;position:relative}header .brand{display:inline-block;position:absolute;left:30px;top:20px;line-height:15px;text-transform:uppercase;padding:7.5px;font-size:1.4em}header .brand:hover{background-color:#ff1d55;color:#fff}header a{color:#fff;text-decoration:none;border-radius:7.5px;padding:7.5px}header a:hover{text-decoration:none;background-color:#fff;color:#ff1d55}header nav ul{padding:0;margin:0;display:block;text-align:right}header nav ul li{padding:0;margin:0 7.5px;display:inline-block}header nav ul li.hamburger{display:none}header nav ul li.hamburger button{position:absolute;right:30px;top:20px;border:0;background-color:#fff;color:#ff1d55;font-weight:700;border-radius:50%;width:30px;height:30px;overflow:hidden;text-align:center;padding:0;margin:0}@media(max-width:900px){header nav ul li{display:none}header nav ul li.hamburger{display:inline-block}}.sidebarNavigation{width:210px;max-width:95%}.sidebarNavigation ul{padding:0;margin:0}.sidebarNavigation ul li{display:block}.sidebarNavigation ul li a{display:block;line-height:1.4em;padding:7.5px}footer{margin:60px auto;display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-line-pack:center;align-content:center;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;max-width:1180px;padding:30px;font-size:12px}footer a:hover{text-decoration:none}footer strong{text-transform:uppercase;margin-bottom:15px;display:block}footer strong a{color:#1d1d1d}footer ul{display:block;padding:0;margin:0}footer ul li{display:block;line-height:1.4em;padding:7.5px 0;margin:0}footer ul li a{color:dimgray}footer .quarter-section{margin-bottom:30px;-ms-flex-preferred-size:25%;flex-basis:25%}@media(max-width:900px){footer .quarter-section{-ms-flex-preferred-size:33%;flex-basis:33%}}@media(max-width:700px){footer .quarter-section{-ms-flex-preferred-size:50%;flex-basis:50%}}@media(max-width:500px){footer .quarter-section{-ms-flex-preferred-size:100%;flex-basis:100%}}.boxes{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-line-pack:center;align-content:center;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;font-size:12px}.boxes .box{-ms-flex-preferred-size:30%;flex-basis:30%;background-color:#f7f7f7;padding:30px;margin:7.5px 0;text-align:left;font-size:12px}.boxes .box h3{font-size:12px;line-height:1em;padding:0;margin:0}.boxes .box a{padding:0;color:#ff1d55}.boxes .box p{color:dimgray;font-size:12px;margin:7.5px 0}@media(max-width:900px){.boxes .box{-ms-flex-preferred-size:45%;flex-basis:45%}}@media(max-width:500px){.boxes .box{-ms-flex-preferred-size:100%;flex-basis:100%}}.fixed-container{position:relative;width:100%;height:300px;margin:auto}amp-img.contain img{object-fit:contain}.footnotes{overflow-wrap:break-word}</style><script data-ad-client=ca-pub-2665873967488848 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script async custom-element=amp-auto-ads src=https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js></script></head><body><main><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">计算机视觉</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://justpic.org title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw"></i>justpic</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/><i class="far fa-folder fa-fw"></i>人工智能</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2014-05-06>2014-05-06</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1241 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 3 分钟&nbsp;<span id=/amp/posts/2014/05/06/ai-collection-2014/ class=leancloud_visitors data-flag-title=计算机视觉>
<i class="far fa-eye fa-fw"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
</span>&nbsp;</span></div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#计算机视觉机器学习相关领域论文和源代码大集合--持续更新>计算机视觉、机器学习相关领域论文和源代码大集合&ndash;持续更新</a><ul><li><a href=#特征提取feature-extraction>特征提取Feature Extraction</a></li><li><a href=#图像分割image-segmentation>图像分割Image Segmentation</a></li><li><a href=#目标检测object-detection>目标检测Object Detection</a></li><li><a href=#显著性检测saliency-detection>显著性检测Saliency Detection</a></li><li><a href=#图像分类聚类image-classification-clustering>图像分类、聚类Image Classification, Clustering</a></li><li><a href=#抠图image-matting>抠图Image Matting</a></li><li><a href=#目标跟踪object-tracking>目标跟踪Object Tracking</a></li><li><a href=#kinect>Kinect</a></li><li><a href=#3d相关>3D相关</a></li><li><a href=#机器学习算法>机器学习算法</a></li><li><a href=#目标行为识别object-action-recognition>目标、行为识别Object, Action Recognition</a></li><li><a href=#图像处理>图像处理</a></li><li><a href=#一些实用工具>一些实用工具</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>计算机视觉论文源代码集锦</p><h2 id=计算机视觉机器学习相关领域论文和源代码大集合--持续更新>计算机视觉、机器学习相关领域论文和源代码大集合&ndash;持续更新</h2><h3 id=特征提取feature-extraction>特征提取Feature Extraction</h3><ol><li>SIFT [<a href=http://www.cs.ubc.ca/~lowe/keypoints/siftDemoV4.zip target=_blank>Demo program</a>][<a href=http://blogs.oregonstate.edu/hess/code/sift/ target=_blank>SIFT Library</a>] [<a href=http://www.vlfeat.org/ target=_blank>VLFeat</a>]</li><li>PCA-SIFT [<a href=http://www.cs.cmu.edu/~yke/pcasift/ target=_blank>Project</a>]</li><li>Affine-SIFT [<a href=http://www.ipol.im/pub/algo/my_affine_sift/ target=_blank>Project</a>]</li><li>SURF [<a href=http://www.chrisevansdev.com/computer-vision-opensurf.html target=_blank>OpenSURF</a>] [<a href=http://www.maths.lth.se/matematiklth/personal/petter/surfmex.php target=_blank>Matlab Wrapper</a>]</li><li>Affine Covariant Features [<a href=http://www.robots.ox.ac.uk/~vgg/research/affine/ target=_blank>Oxford project</a>]</li><li>MSER [<a href=http://www.robots.ox.ac.uk/~vgg/research/affine/ target=_blank>Oxford project</a>] [<a href=http://www.vlfeat.org/ target=_blank>VLFeat</a>]</li><li>Geometric Blur [<a href=http://www.robots.ox.ac.uk/~vgg/software/MKL/ target=_blank>Code</a>]</li><li>Local Self-Similarity Descriptor [<a href=http://www.robots.ox.ac.uk/~vgg/software/SelfSimilarity/ target=_blank>Oxford implementation</a>]</li><li>Global and Efficient Self-Similarity [<a href=http://www.vision.ee.ethz.ch/~calvin/gss/selfsim_release1.0.tgz target=_blank>Code</a>]</li><li>Histogram of Oriented Graidents [<a href=http://www.navneetdalal.com/software target=_blank>INRIA Object Localization Toolkit</a>] [<a href=http://www.computing.edu.au/~12482661/hog.html target=_blank>OLT toolkit for Windows</a>]</li><li>Shape Context [<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sc_digits.html target=_blank>Project</a>]</li><li>Color Descriptor [<a href=http://koen.me/research/colordescriptors/ target=_blank>Project</a>]</li><li>Pyramids of Histograms of Oriented Gradients [<a href=http://www.robots.ox.ac.uk/~vgg/research/caltech/phog/phog.zip target=_blank>Code</a>]</li><li>Space-Time Interest Points (STIP) [<a href=http://www.nada.kth.se/cvap/abstracts/cvap284.html target=_blank>Project</a>] [<a href=http://www.irisa.fr/vista/Equipe/People/Laptev/download/stip-1.1-winlinux.zip target=_blank>Code</a>]</li><li>Boundary Preserving Dense Local Regions[<a href=http://vision.cs.utexas.edu/projects/bplr/bplr.html target=_blank>Project</a>]</li><li>Weighted Histogram[<a href=http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/whistc.tar.gz target=_blank>Code</a>]</li><li>Histogram-based Interest Points Detectors[<a href=http://www.cs.nthu.edu.tw/~htchen/hipd_cvpr09.pdf target=_blank>Paper</a>][<a href=http://740-2.cs.nthu.edu.tw/~htchen/hipd/hist_corner.zip target=_blank>Code</a>]</li><li>An OpenCV – C++ implementation of Local Self Similarity Descriptors [<a href=http://intuitionlogic.com/post/2011/04/11/A-OpenCV-C++-implementation-of-Local-Self-Similarity-Descriptors.aspx target=_blank>Project</a>]</li><li>Fast Sparse Representation with Prototypes[<a href=http://faculty.ucmerced.edu/mhyang/cvpr10_fsr.html target=_blank>Project</a>]</li><li>Corner Detection [<a href=http://kiwi.cs.dal.ca/~dparks/CornerDetection/index.htm target=_blank>Project</a>]</li><li>AGAST Corner Detector: faster than FAST and even FAST-ER[<a href=http://www6.in.tum.de/Main/ResearchAgast target=_blank>Project</a>]<br>1</li></ol><h3 id=图像分割image-segmentation>图像分割Image Segmentation</h3><ol><li>Normalized Cut [<a href=http://www.cis.upenn.edu/~jshi/software/ target=_blank>Matlab code</a>]</li><li>Gerg Mori’ Superpixel code [<a href=http://www.cs.sfu.ca/~mori/research/superpixels/ target=_blank>Matlab code</a>]</li><li>Efficient Graph-based Image Segmentation [<a href=http://people.cs.uchicago.edu/~pff/segment/ target=_blank>C++ code</a>] [<a href=http://www.mathworks.com/matlabcentral/fileexchange/25866-efficient-graph-based-image-segmentation target=_blank>Matlab wrapper</a>]</li><li>Mean-Shift Image Segmentation [<a href=http://coewww.rutgers.edu/riul/research/code/EDISON/index.html target=_blank>EDISON C++ code</a>] [<a href=http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/edison_matlab_interface.tar.gz target=_blank>Matlab wrapper</a>]</li><li>OWT-UCM Hierarchical Segmentation [<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html target=_blank>Resources</a>]</li><li>Turbepixels [<a href=http://www.cs.toronto.edu/~babalex/turbopixels_code.tar.gz target=_blank>Matlab code 32bit</a>] [<a href=http://www.cs.toronto.edu/~babalex/TurboPixels64.rar target=_blank>Matlab code 64bit</a>] [<a href=http://www.cs.toronto.edu/~babalex/superpixels_update.tgz target=_blank>Updated code</a>]</li><li>Quick-Shift[<a href=http://www.vlfeat.org/overview/quickshift.html target=_blank>VLFeat</a>]</li><li>SLIC Superpixels [<a href=http://ivrgwww.epfl.ch/supplementary_material/RK_SLICSuperpixels/index.html target=_blank>Project</a>]</li><li>Segmentation by Minimum Code Length [<a href=http://perception.csl.uiuc.edu/coding/image_segmentation/ target=_blank>Project</a>]</li><li>Biased Normalized Cut [<a href=http://www.cs.berkeley.edu/~smaji/projects/biasedNcuts/ target=_blank>Project</a>]</li><li>Segmentation Tree [<a href=http://vision.ai.uiuc.edu/segmentation target=_blank>Project</a>]</li><li>Entropy Rate Superpixel Segmentation [<a href=http://www.umiacs.umd.edu/~mingyliu/src/ers_matlab_wrapper_v0.1.zip target=_blank>Code</a>]</li><li>Fast Approximate Energy Minimization via Graph Cuts[<a href=http://www.csd.uwo.ca/faculty/olga/Papers/pami01_final.pdf target=_blank>Paper</a>][<a href=http://vision.csd.uwo.ca/code/gco-v3.0.zip target=_blank>Code</a>]</li><li>Efﬁcient Planar Graph Cuts with Applications in Computer Vision[<a href=http://www.csd.uwo.ca/~schmidtf/pdf/schmidt_et_al_cvpr09.pdf target=_blank>Paper</a>][<a href=http://vision.csd.uwo.ca/code/PlanarCut-v1.0.zip target=_blank>Code</a>]</li><li>Isoperimetric Graph Partitioning for Image Segmentation[<a href=http://www.cns.bu.edu/~lgrady/grady2006isoperimetric.pdf target=_blank>Paper</a>][<a href=http://www.cns.bu.edu/~lgrady/grady2006isoperimetric_code.zip target=_blank>Code</a>]</li><li>Random Walks for Image Segmentation[<a href=http://www.cns.bu.edu/~lgrady/grady2006random.pdf target=_blank>Paper</a>][<a href=http://www.cns.bu.edu/~lgrady/random_walker_matlab_code.zip target=_blank>Code</a>]</li><li>Blossom V: A new implementation of a minimum cost perfect matching algorithm[<a href=http://pub.ist.ac.at/~vnk/software/blossom5-v2.03.src.tar.gz%20%20http:/pub.ist.ac.at/~vnk/software/blossom5-v2.03.src.tar.gz target=_blank>Code</a>]</li><li>An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[<a href=http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf target=_blank>Paper</a>][<a href=http://pub.ist.ac.at/~vnk/software/maxflow-v3.01.src.tar.gz target=_blank>Code</a>]</li><li>Geodesic Star Convexity for Interactive Image Segmentation[<a href=http://www.robots.ox.ac.uk/~vgg/software/iseg/ target=_blank>Project</a>]</li><li>Contour Detection and Image Segmentation Resources[<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html target=_blank>Project</a>][<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_source.tgz target=_blank>Code</a>]</li><li>Biased Normalized Cuts[<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/biasedNcuts/ target=_blank>Project</a>]</li><li>Max-flow/min-cut[<a href=http://vision.csd.uwo.ca/code/ target=_blank>Project</a>]</li><li>Chan-Vese Segmentation using Level Set[<a href=http://www.ipol.im/pub/art/2012/g-cv/ target=_blank>Project</a>]</li><li>A Toolbox of Level Set Methods[<a href=http://www.cs.ubc.ca/~mitchell/ToolboxLS/index.html target=_blank>Project</a>]</li><li>Re-initialization Free Level Set Evolution via Reaction Diffusion[<a href=http://www4.comp.polyu.edu.hk/~cslzhang/RD/RD.htm target=_blank>Project</a>]</li><li>Improved C-V active contour model[<a href=http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/ICV.pdf target=_blank>Paper</a>][<a href=http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/ICV.rar target=_blank>Code</a>]</li><li>A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[<a href=http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/ICIP10_SVMLS.pdf target=_blank>Paper</a>][<a href=http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/SVMLS_v0.rar target=_blank>Code</a>]</li><li>Level Set Method Research by Chunming Li[<a href=http://www.engr.uconn.edu/~cmli/ target=_blank>Project</a>]</li></ol><h3 id=目标检测object-detection>目标检测Object Detection</h3><ol><li>A simple object detector with boosting [<a href=http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html target=_blank>Project</a>]</li><li>INRIA Object Detection and Localization Toolkit [<a href=http://pascal.inrialpes.fr/soft/olt/ target=_blank>Project</a>]</li><li>Discriminatively Trained Deformable Part Models [<a href=http://people.cs.uchicago.edu/~pff/latent/ target=_blank>Project</a>]</li><li>Cascade Object Detection with Deformable Part Models [<a href=http://people.cs.uchicago.edu/~rbg/star-cascade/ target=_blank>Project</a>]</li><li>Poselet [<a href=http://www.eecs.berkeley.edu/~lbourdev/poselets/ target=_blank>Project</a>]</li><li>Implicit Shape Model [<a href=http://www.vision.ee.ethz.ch/~bleibe/code/ism.html target=_blank>Project</a>]</li><li>Viola and Jones’s Face Detection [<a href=http://pr.willowgarage.com/wiki/Face_detection target=_blank>Project</a>]</li><li>Bayesian Modelling of Dyanmic Scenes for Object Detection[<a href=http://vision.eecs.ucf.edu/papers/01512057.pdf target=_blank>Paper</a>][<a href=http://vision.eecs.ucf.edu/Code/Background.zip target=_blank>Code</a>]</li><li>Hand detection using multiple proposals[<a href=http://www.robots.ox.ac.uk/~vgg/software/hands/ target=_blank>Project</a>]</li><li>Color Constancy, Intrinsic Images, and Shape Estimation[<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/reconstruction/BarronMalikECCV2012.pdf target=_blank>Paper</a>][<a href=http://www.cs.berkeley.edu/~barron/BarronMalikECCV2012_code.zip target=_blank>Code</a>]</li><li>Discriminatively trained deformable part models[<a href=http://people.cs.uchicago.edu/~rbg/latent/ target=_blank>Project</a>]</li><li>Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [<a href=http://campar.cs.tum.edu/Main/StefanHinterstoisser target=_blank>Project</a>]</li><li>Image Processing On Line[<a href=http://www.ipol.im/ target=_blank>Project</a>]</li><li>Robust Optical Flow Estimation[<a href=http://www.ipol.im/pub/pre/21/ target=_blank>Project</a>]</li><li>Where’s Waldo: Matching People in Images of Crowds[<a href=http://homes.cs.washington.edu/~rahul/data/WheresWaldo.html target=_blank>Project</a>]</li></ol><h3 id=显著性检测saliency-detection>显著性检测Saliency Detection</h3><ol><li>Itti, Koch, and Niebur’ saliency detection [<a href=http://www.saliencytoolbox.net/ target=_blank>Matlab code</a>]</li><li>Frequency-tuned salient region detection [<a href=http://ivrgwww.epfl.ch/supplementary_material/RK_CVPR09/index.html target=_blank>Project</a>]</li><li>Saliency detection using maximum symmetric surround [<a href=http://ivrg.epfl.ch/supplementary_material/RK_ICIP2010/index.html target=_blank>Project</a>]</li><li>Attention via Information Maximization [<a href=http://www.cse.yorku.ca/~neil/AIM.zip target=_blank>Matlab code</a>]</li><li>Context-aware saliency detection [<a href=http://webee.technion.ac.il/labs/cgm/Computer-Graphics-Multimedia/Software/Saliency/Saliency.html target=_blank>Matlab code</a>]</li><li>Graph-based visual saliency [<a href=http://www.klab.caltech.edu/~harel/share/gbvs.php target=_blank>Matlab code</a>]</li><li>Saliency detection: A spectral residual approach. [<a href=http://www.klab.caltech.edu/~xhou/projects/spectralResidual/spectralresidual.html target=_blank>Matlab code</a>]</li><li>Segmenting salient objects from images and videos. [<a href=http://www.cse.oulu.fi/MVG/Downloads/saliency target=_blank>Matlab code</a>]</li><li>Saliency Using Natural statistics. [<a href=http://cseweb.ucsd.edu/~l6zhang/ target=_blank>Matlab code</a>]</li><li>Discriminant Saliency for Visual Recognition from Cluttered Scenes. [<a href=http://www.svcl.ucsd.edu/projects/saliency/ target=_blank>Code</a>]</li><li>Learning to Predict Where Humans Look [<a href=http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html target=_blank>Project</a>]</li><li>Global Contrast based Salient Region Detection [<a href=http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/ target=_blank>Project</a>]</li><li>Bayesian Saliency via Low and Mid Level Cues[<a href=http://ice.dlut.edu.cn/lu/Project/TIP_scm/TIP_scm.htm target=_blank>Project</a>]</li><li>Top-Down Visual Saliency via Joint CRF and Dictionary Learning [<a href=http://faculty.ucmerced.edu/mhyang/papers/cvpr12a.pdf target=_blank>Paper</a>] [<a href=http://faculty.ucmerced.edu/mhyang/code/top-down-saliency.zip target=_blank>Code</a>]</li></ol><h3 id=图像分类聚类image-classification-clustering>图像分类、聚类Image Classification, Clustering</h3><ol><li>Pyramid Match [<a href=http://people.csail.mit.edu/jjl/libpmk/ target=_blank>Project</a>]</li><li>Spatial Pyramid Matching [<a href=http://www.cs.unc.edu/~lazebnik/research/SpatialPyramid.zip target=_blank>Code</a>]</li><li>Locality-constrained Linear Coding [<a href=http://www.ifp.illinois.edu/~jyang29/LLC.htm target=_blank>Project</a>] [<a href=http://www.ifp.illinois.edu/~jyang29/codes/CVPR10-LLC.rar target=_blank>Matlab code</a>]</li><li>Sparse Coding [<a href=http://www.ifp.illinois.edu/~jyang29/ScSPM.htm target=_blank>Project</a>] [<a href=http://www.ifp.illinois.edu/~jyang29/codes/CVPR09-ScSPM.rar target=_blank>Matlab code</a>]</li><li>Texture Classification [<a href=http://www.robots.ox.ac.uk/~vgg/research/texclass/index.html target=_blank>Project</a>]</li><li>Multiple Kernels for Image Classification [<a href=http://www.robots.ox.ac.uk/~vgg/software/MKL/ target=_blank>Project</a>]</li><li>Feature Combination [<a href=http://www.vision.ee.ethz.ch/~pgehler/projects/iccv09/index.html target=_blank>Project</a>]</li><li>SuperParsing [<a href=http://www.cs.unc.edu/~jtighe/Papers/ECCV10/eccv10-jtighe-code.zip target=_blank>Code</a>]</li><li>Large Scale Correlation Clustering Optimization[<a href=http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/LargeScaleCC1.0.tar.gz target=_blank>Matlab code</a>]</li><li>Detecting and Sketching the Common[<a href=http://www.wisdom.weizmann.ac.il/~vision/SketchTheCommon target=_blank>Project</a>]</li><li>Self-Tuning Spectral Clustering[<a href=http://www.vision.caltech.edu/lihi/Demos/SelfTuningClustering.html target=_blank>Project</a>][<a href=http://www.vision.caltech.edu/lihi/Demos/SelfTuning/ZPclustering.zip target=_blank>Code</a>]</li><li>User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[<a href=http://www.wisdom.weizmann.ac.il/~levina/papers/assisted-eccv04.pdf target=_blank>Paper</a>][<a href=http://www.wisdom.weizmann.ac.il/~levina/papers/reflections.zip target=_blank>Code</a>]</li><li>Filters for Texture Classification[<a href=http://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html#download target=_blank>Project</a>]</li><li>Multiple Kernel Learning for Image Classification[<a href=http://www.robots.ox.ac.uk/~vgg/software/MKL/ target=_blank>Project</a>]</li><li>SLIC Superpixels[<a href=http://ivrg.epfl.ch/supplementary_material/RK_SLICSuperpixels/ target=_blank>Project</a>]</li></ol><h3 id=抠图image-matting>抠图Image Matting</h3><ol><li>A Closed Form Solution to Natural Image Matting [<a href=http://people.csail.mit.edu/alevin/matting.tar.gz target=_blank>Code</a>]</li><li>Spectral Matting [<a href=http://www.vision.huji.ac.il/SpectralMatting/ target=_blank>Project</a>]</li><li>Learning-based Matting [<a href=http://www.mathworks.com/matlabcentral/fileexchange/31412 target=_blank>Code</a>]</li></ol><h3 id=目标跟踪object-tracking>目标跟踪Object Tracking</h3><ol><li>A Forest of Sensors – Tracking Adaptive Background Mixture Models [<a href=http://www.ai.mit.edu/projects/vsam/Tracking/index.html target=_blank>Project</a>]</li><li>Object Tracking via Partial Least Squares Analysis[<a href=http://faculty.ucmerced.edu/mhyang/papers/tip12_pls_tracking.pdf target=_blank>Paper</a>][<a href=http://faculty.ucmerced.edu/mhyang/code/PLS_tracker_tip.zip target=_blank>Code</a>]</li><li>Robust Object Tracking with Online Multiple Instance Learning[<a href=http://faculty.ucmerced.edu/mhyang/papers/pami11b.pdf target=_blank>Paper</a>][<a href=http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml target=_blank>Code</a>]</li><li>Online Visual Tracking with Histograms and Articulating Blocks[<a href=http://www.cise.ufl.edu/~smshahed/tracking.htm target=_blank>Project</a>]</li><li>Incremental Learning for Robust Visual Tracking[<a href=http://www.cs.toronto.edu/~dross/ivt/ target=_blank>Project</a>]</li><li>Real-time Compressive Tracking[<a href=http://www4.comp.polyu.edu.hk/~cslzhang/CT/CT.htm target=_blank>Project</a>]</li><li>Robust Object Tracking via Sparsity-based Collaborative Model[<a href=http://faculty.ucmerced.edu/mhyang/project/cvpr12_scm.htm target=_blank>Project</a>]</li><li>Visual Tracking via Adaptive Structural Local Sparse Appearance Model[<a href=http://faculty.ucmerced.edu/mhyang/project/cvpr12_jia_project.htm target=_blank>Project</a>]</li><li>Online Discriminative Object Tracking with Local Sparse Representation[<a href=http://faculty.ucmerced.edu/mhyang/papers/wacv12a.pdf target=_blank>Paper</a>][<a href=http://faculty.ucmerced.edu/mhyang/code/wacv12a_code.zip target=_blank>Code</a>]</li><li>Superpixel Tracking[<a href=http://faculty.ucmerced.edu/mhyang/papers/iccv11a.html target=_blank>Project</a>]</li><li>Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[<a href=http://faculty.ucmerced.edu/mhyang/papers/bmvc11a.pdf target=_blank>Paper</a>][<a href=http://faculty.ucmerced.edu/mhyang/code/BMVC11-HSSL-package.zip target=_blank>Code</a>]</li><li>Online Multiple Support Instance Tracking [<a href=http://faculty.ucmerced.edu/mhyang/papers/fg11a.pdf target=_blank>Paper</a>][<a href=http://faculty.ucmerced.edu/mhyang/code/fg11_omsit.zip target=_blank>Code</a>]</li><li>Visual Tracking with Online Multiple Instance Learning[<a href=http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml target=_blank>Project</a>]</li><li>Object detection and recognition[<a href=http://c2inet.sce.ntu.edu.sg/Jianxin/ target=_blank>Project</a>]</li><li>Compressive Sensing Resources[<a href=http://dsp.rice.edu/cs target=_blank>Project</a>]</li><li>Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[<a href=http://www.robots.ox.ac.uk/~cbibby/index.shtml target=_blank>Project</a>]</li><li>Tracking-Learning-Detection[<a href=http://info.ee.surrey.ac.uk/Personal/Z.Kalal/ target=_blank>Project</a>][<a href=https://github.com/arthurv/OpenTLD target=_blank>OpenTLD/C++ Code</a>]</li><li>the HandVu：vision-based hand gesture interface[<a href=http://ilab.cs.ucsb.edu/index.php/component/content/article/12/29 target=_blank>Project</a>]</li></ol><h3 id=kinect>Kinect</h3><ol><li>Kinect toolbox[<a href=http://kinecttoolbox.codeplex.com/ target=_blank>Project</a>]</li><li>OpenNI[<a href=http://www.openni.org/ target=_blank>Project</a>]</li><li>zouxy09 CSDN Blog[<a href=http://blog.csdn.net/zouxy09/article/details/814561 target=_blank>Resource</a>]</li></ol><h3 id=3d相关>3D相关</h3><ol><li>3D Reconstruction of a Moving Object[<a href=http://www.wisdom.weizmann.ac.il/~ronen/papers/Simakov%20Frolova%20Basri%20-%20Dense%20Shape%20Reconstruction%20Under%20Arbitrary%20Unknown%20Lighting.pdf target=_blank>Paper</a>] [<a href=http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/SFB_matlab1.0.tar.gz target=_blank>Code</a>]</li><li>Shape From Shading Using Linear Approximation[<a href=http://vision.eecs.ucf.edu/shadsrc.html target=_blank>Code</a>]</li><li>Combining Shape from Shading and Stereo Depth Maps[<a href=http://vision.eecs.ucf.edu/combsrc.html target=_blank>Project</a>][<a href=http://vision.eecs.ucf.edu/projects/ShapeFromShading/combine.tar.Z target=_blank>Code</a>]</li><li>Shape from Shading: A Survey[<a href=http://vision.eecs.ucf.edu/papers/shah/99/ZTCS99.pdf target=_blank>Paper</a>][<a href=http://vision.eecs.ucf.edu/projects/ShapeFromShading/SFS_Survey_1_00.tar.gz target=_blank>Code</a>]</li><li>A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[<a href=http://lear.inrialpes.fr/people/klaeser/research_hog3d target=_blank>Project</a>][<a href=http://lear.inrialpes.fr/people/klaeser/software_3d_video_descriptor target=_blank>Code</a>]</li><li>Multi-camera Scene Reconstruction via Graph Cuts[<a href=http://www.cs.cornell.edu/~rdz/papers/kz-eccv02-recon.pdf target=_blank>Paper</a>][<a href=http://pub.ist.ac.at/~vnk/software/match-v3.4.src.tar.gz target=_blank>Code</a>]</li><li>A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[<a href=http://www.cs.ucf.edu/~vision target=_blank>Paper</a>][<a href=http://www.ee.cityu.edu.hk/~syyuen/Public/SfS/PRL_Perspective_FMM.zip target=_blank>Code</a>]</li><li>Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/reconstruction/ target=_blank>Project</a>]</li><li>Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[<a href=http://faculty.ucmerced.edu/mhyang/code/PackagedTrackingCode.tar.gz target=_blank>Code</a>]</li><li>Learning 3-D Scene Structure from a Single Still Image[<a href=http://ai.stanford.edu/~asaxena/reconstruction3d/ target=_blank>Project</a>]</li></ol><h3 id=机器学习算法>机器学习算法</h3><ol><li>Matlab class for computing Approximate Nearest Nieghbor (ANN) [<a href=http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/ann_wrapper_Mar2012.tar.gz target=_blank>Matlab class</a> providing interface to<a href=http://www.cs.umd.edu/~mount/ANN/ target=_blank>ANN library</a>]</li><li>Random Sampling[<a href=http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/weight_sample.tar.gz target=_blank>code</a>]</li><li>Probabilistic Latent Semantic Analysis (pLSA)[<a href=http://www.robots.ox.ac.uk/~vgg/software/pLSA/pLSA_demo.tgz target=_blank>Code</a>]</li><li>FASTANN and FASTCLUSTER for approximate k-means (AKM)[<a href=http://www.robots.ox.ac.uk/~vgg/software/fastann/ target=_blank>Project</a>]</li><li>Fast Intersection / Additive Kernel SVMs[<a href=http://www.cs.berkeley.edu/~smaji/projects/fiksvm/ target=_blank>Project</a>]</li><li>SVM[<a href=http://osmot.cs.cornell.edu/svm_light/ target=_blank>Code</a>]</li><li>Ensemble learning[<a href=http://c2inet.sce.ntu.edu.sg/Jianxin/ target=_blank>Project</a>]</li><li>Deep Learning[<a href=http://deeplearning.net/ target=_blank>Net</a>]</li><li>Deep Learning Methods for Vision[<a href=http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/ target=_blank>Project</a>]</li><li>Neural Network for Recognition of Handwritten Digits[<a href=http://www.codeproject.com/KB/library/NeuralNetRecognition.aspx target=_blank>Project</a>]</li><li>Training a deep autoencoder or a classifier on MNIST digits[<a href=http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html target=_blank>Project</a>]</li><li>THE MNIST DATABASE of handwritten digits[<a href=http://yann.lecun.com/exdb/mnist/ target=_blank>Project</a>]</li><li>Ersatz：deep neural networks in the cloud[<a href=http://www.ersatz1.com/ target=_blank>Project</a>]</li><li>Deep Learning [<a href=http://www.cs.nyu.edu/~yann/research/deep/ target=_blank>Project</a>]</li><li>sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[<a href=http://www.ics.forth.gr/~lourakis/sparseLM/ target=_blank>Project</a>]</li><li>Weka 3: Data Mining Software in Java[<a href=http://www.cs.waikato.ac.nz/ml/weka/ target=_blank>Project</a>]</li><li>Invited talk “A Tutorial on Deep Learning” by Dr. Kai Yu (余凯)[<a href=http://vipl.ict.ac.cn/News/academic-report-tutorial-deep-learning-dr-kai-yu target=_blank>Video</a>]</li><li>CNN – Convolutional neural network class[<a href=http://www.mathworks.cn/matlabcentral/fileexchange/24291 target=_blank>Matlab Tool</a>]</li><li>Yann LeCun’s Publications[<a href=http://yann.lecun.com/exdb/publis/index.html#lecun-98 target=_blank>Wedsite</a>]</li><li>LeNet-5, convolutional neural networks[<a href=http://yann.lecun.com/exdb/lenet/index.html target=_blank>Project</a>]</li><li>Training a deep autoencoder or a classifier on MNIST digits[<a href=http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html target=_blank>Project</a>]</li><li>Deep Learning 大牛Geoffrey E. Hinton’s HomePage[<a href=http://www.cs.toronto.edu/~hinton/ target=_blank>Website</a>]</li></ol><h3 id=目标行为识别object-action-recognition>目标、行为识别Object, Action Recognition</h3><ol><li>Action Recognition by Dense Trajectories[<a href=http://lear.inrialpes.fr/people/wang/dense_trajectories target=_blank>Project</a>][<a href=http://lear.inrialpes.fr/people/wang/download/dense_trajectory_release.tar.gz target=_blank>Code</a>]</li><li>Action Recognition Using a Distributed Representation of Pose and Appearance[<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/action/ target=_blank>Project</a>]</li><li>Recognition Using Regions[<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/glam-cvpr09.pdf target=_blank>Paper</a>][<a href=http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/glam_cvpr09_v2.zip target=_blank>Code</a>]</li><li>2D Articulated Human Pose Estimation[<a href=http://www.vision.ee.ethz.ch/~calvin/articulated_human_pose_estimation_code/ target=_blank>Project</a>]</li><li>Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[<a href=http://faculty.ucmerced.edu/mhyang/papers/cvpr07a.pdf target=_blank>Paper</a>][<a href=http://www.cise.ufl.edu/~smshahed/cvpr07_fast_human_pose.zip target=_blank>Code</a>]</li><li>Estimating Human Pose from Occluded Images[<a href=http://faculty.ucmerced.edu/mhyang/papers/accv09a.pdf target=_blank>Paper</a>][<a href=http://faculty.ucmerced.edu/mhyang/code/accv09_pose.zip target=_blank>Code</a>]</li><li>Quasi-dense wide baseline matching[<a href=http://www.ee.oulu.fi/~jkannala/quasidense/quasidense.html target=_blank>Project</a>]</li><li>ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[<a href=http://gesture.chalearn.org/data/sample-code target=_blank>Prpject</a>]</li></ol><h3 id=图像处理>图像处理</h3><ol><li>Distance Transforms of Sampled Functions[<a href=http://cs.brown.edu/~pff/dt/ target=_blank>Project</a>]</li><li>The Computer Vision Homepage[<a href=http://www.cs.cmu.edu/~cil/vision.html target=_blank>Project</a>]</li></ol><h3 id=一些实用工具>一些实用工具</h3><ol><li>EGT: a Toolbox for Multiple View Geometry and Visual Servoing[<a href=http://egt.dii.unisi.it/ target=_blank>Project</a>] [<a href=http://egt.dii.unisi.it/download/EGT_v1p3.zip target=_blank>Code</a>]</li><li>a development kit of matlab mex functions for OpenCV library[<a href=http://www.cs.stonybrook.edu/~kyamagu/mexopencv/ target=_blank>Project</a>]</li><li>Fast Artificial Neural Network Library[<a href=http://leenissen.dk/fann/wp/ target=_blank>Project</a>]</li><li><a href=www.cvchina.info/2011/09/05/uiuc-cod/ rel>www.cvchina.info/2011/09/05/uiuc-cod/</a></li></ol></div><div class=content id=content></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2014-05-06</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉 data-via=xxxx data-hashtags=计算机视觉,AI><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-hashtag=计算机视觉><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="分享到 WhatsApp" data-sharer=whatsapp data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉 data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉><i data-svg-src=/lib/simple-icons/icons/line.min.svg></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉 data-ralateuid=xxxx><i class="fab fa-weibo fa-fw"></i></a><a href=javascript:void(0); title="分享到 Myspace" data-sharer=myspace data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉 data-description><i data-svg-src=/lib/simple-icons/icons/myspace.min.svg></i></a><a href=javascript:void(0); title="分享到 Blogger" data-sharer=blogger data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉 data-description><i class="fab fa-blogger fa-fw"></i></a><a href=javascript:void(0); title="分享到 Evernote" data-sharer=evernote data-url=https://justpic.org/amp/posts/2014/05/06/ai-collection-2014/ data-title=计算机视觉><i class="fab fa-evernote fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/>计算机视觉</a>,&nbsp;<a href=/tags/ai/>AI</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/amp/posts/2014/04/27/tornado-%E5%9B%BD%E9%99%85%E5%8C%96%E5%AE%9E%E8%B7%B5/ class=prev rel=prev title="tornado 国际化实践"><i class="fas fa-angle-left fa-fw"></i>tornado 国际化实践</a>
<a href=/amp/posts/2014/05/06/matlab-mixed-c/ class=next rel=next title="Matlab C混合编程">Matlab C混合编程<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article><div class=adsblock><script async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><ins class=adsbygoogle style=display:block data-ad-format=fluid data-ad-layout-key=-73+ed+2i-1n-4w data-ad-client=ca-pub-2665873967488848 data-ad-slot=2692397084></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({});</script></div></main><amp-analytics type=googleanalytics><script type=application/json>{"vars":{"account":""},"triggers":{"trackEvent":{"on":"click","request":"event","selector":"body","vars":{"eventAction":"click","eventCategory":"body-click"}},"trackPageview":{"on":"visible","request":"pageview"}}}</script></amp-analytics><amp-auto-ads type=adsense data-ad-client=ca-pub-2665873967488848></amp-auto-ads></body></html>